# Web Scraping - Proyecto Individual

## √çndice
1. [Estructura de la base de datos MySQL](#estructura-de-la-base-de-datos-mysql) üìä
2. [Librer√≠as de Web Scraping, Beatiful Soup, Scrapy o Selenium ](#librer√≠as-de-web-scraping-beatiful-soup-o-scrapy) üõ†Ô∏è
3. [Estructura del Proyecto](#estructura-del-proyecto) üìÅ
4. [Documentaci√≥n del Proyecto de Web Scraping con Django](#documentaci√≥n-del-proyecto-de-web-scraping-con-django) üìÑ
    - [1. Visi√≥n General del Proyecto](#1-visi√≥n-general-del-proyecto)
    - [2. Stack Tecnol√≥gico](#2-stack-tecnol√≥gico)
    - [3. Estructura del Proyecto](#3-estructura-del-proyecto)
    - [4. Instalaci√≥n y Configuraci√≥n](#4-instalaci√≥n-y-configuraci√≥n)
    - [5. Componentes Principales](#5-componentes-principales)
    - [6. Modelos de Datos](#6-modelos-de-datos)
    - [7. Vistas y URLs (endpoints)](#7-vistas-y-urls-endpoints)
    - [8. Visualizaci√≥n de Datos](#8-visualizaci√≥n-de-datos)
    - [9. Test](#9-test)
    - [10. Despliegue](#10-despliegue)
    - [11. Mantenimiento y Actualizaciones](#11-mantenimiento-y-actualizaciones)
    - [12. Soluci√≥n de Problemas](#12-soluci√≥n-de-problemas)
    - [13. Mejoras Futuras](#13-mejoras-futuras)
    - [14. Contribuci√≥n](#14-contribuci√≥n)
    - [15. Licencia](#15-licencia)
    - [16. Informaci√≥n de Contacto](#16-informaci√≥n-de-contacto)

## Estructura de la base de datos MySQL üìä

```sql
DROP DATABASE IF EXISTS quotes;
CREATE DATABASE quotes;
USE quotes;

CREATE TABLE authors (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100) NOT NULL UNIQUE,
    about TEXT,
    born DATE,
    birth_place VARCHAR(100),
    about_page_url VARCHAR(200)  -- Campo para almacenar la URL de la p√°gina "about"
);

CREATE TABLE quotes (
    id INT AUTO_INCREMENT PRIMARY KEY,
    text TEXT NOT NULL,
    author_id INT,
    FOREIGN KEY (author_id) REFERENCES authors(id)
);

CREATE TABLE tags (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(50) NOT NULL UNIQUE
);

CREATE TABLE quote_tags (
    id INT AUTO_INCREMENT PRIMARY KEY,
    quote_id INT,
    tag_id INT,
    FOREIGN KEY (quote_id) REFERENCES quotes(id),
    FOREIGN KEY (tag_id) REFERENCES tags(id),
    UNIQUE (quote_id, tag_id)  -- Asegura que no haya duplicados en la combinaci√≥n de quote_id y tag_id
);

```

![image](https://github.com/user-attachments/assets/25daa383-fe4f-4b33-ba7e-9049f1a4ed9d)


### Librer√≠as de Web Scraping, Beatiful Soup, Scrapy o Selenium üõ†Ô∏è

En el archivo `web_scraper.py` estamos usando la librer√≠a **Beautiful Soup** para hacer web scraping. Cada librer√≠a tiene sus propias fortalezas:

**Beautiful Soup:**

- Es m√°s f√°cil de aprender y usar para proyectos peque√±os a medianos.
- Es excelente para analizar HTML y XML.
- Se integra bien con `requests` para hacer peticiones HTTP.
- Es ideal para scraping de pocas p√°ginas o sitios web simples.

**Scrapy:**

- Es un framework m√°s completo y potente para web scraping a gran escala.
- Tiene mejor rendimiento para proyectos grandes.
- Incluye caracter√≠sticas como manejo de sesiones, pipelines de datos, y exportaci√≥n de datos.
- Es mejor para scraping de m√∫ltiples p√°ginas o sitios web complejos.

**Selenium:**

- Est√° dise√±ado principalmente para pruebas automatizadas de aplicaciones web, pero tambi√©n es √∫til para scraping.
- Permite la interacci√≥n con sitios web din√°micos y cargados por JavaScript.
- Es ideal para situaciones en las que el contenido se carga de forma din√°mica o requiere interacci√≥n con formularios y botones.
- Puede ser m√°s lento comparado con Beautiful Soup y Scrapy, debido a la necesidad de controlar un navegador.

En este caso, Beautiful Soup es una buena elecci√≥n porque el sitio que estamos scrapeando es relativamente simple y no necesitamos las caracter√≠sticas avanzadas de Scrapy o la capacidad de interacci√≥n de Selenium. Sin embargo, si nos encontr√°ramos con un sitio que carga contenido din√°micamente mediante JavaScript o que requiere interacci√≥n, Selenium ser√≠a una opci√≥n a considerar.

## Estructura del Proyecto üìÅ

```markdown
django-web-scraping/
‚îÇ
‚îú‚îÄ‚îÄ manage.py
‚îú‚îÄ‚îÄ xyz_quotes/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îú‚îÄ‚îÄ urls.py
‚îÇ   ‚îú‚îÄ‚îÄ wsgi.py
‚îÇ
‚îú‚îÄ‚îÄ quotes/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ admin.py
‚îÇ   ‚îú‚îÄ‚îÄ apps.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ views.py
‚îÇ   ‚îú‚îÄ‚îÄ urls.py
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quote_list.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ author_detail.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tag_detail.html
‚îÇ   ‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ styles.css
‚îÇ   ‚îú‚îÄ‚îÄ management/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ commands/
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îú‚îÄ‚îÄ web_scraper.py
‚îÇ           ‚îú‚îÄ‚îÄ data_import.py
‚îÇ           ‚îú‚îÄ‚îÄ update_quotes.py
‚îÇ
‚îî‚îÄ‚îÄ requirements.txt

```

# Documentaci√≥n del Proyecto de Web Scraping con Django üìÑ

## 1. Visi√≥n General del Proyecto

- Nombre del Proyecto: Web Scraping con Django
- Prop√≥sito: Extraer citas y informaci√≥n de autores de [quotes.toscrape.com](http://quotes.toscrape.com/) y presentarlas en una aplicaci√≥n web
- Caracter√≠sticas Principales: Web scraping, almacenamiento de datos, presentaci√≥n de datos, visualizaci√≥n de datos

## 2. Stack Tecnol√≥gico

- Backend: Django
- Base de Datos: MySQL
- Frontend: Plantillas de Django, HTML, CSS (Bootstrap).
- Extracci√≥n de Datos: Beautiful Soup, Requests
- Visualizaci√≥n de Datos: Matplotlib, Seaborn

## 3. Estructura del Proyecto

![image](https://github.com/user-attachments/assets/2ab08321-7272-4935-a787-d3447d52c2fa)

## 4. Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- Gestor de bases de datos (MySQL Workbench)
- Git (para clonar el repositorio)

### Configuraci√≥n del entorno

1. Clonar el repositorio:
    
    ```
    git clone <https://github.com/AI-School-F5-P3/WebScraping_Angelmaria.git>
    cd WebScraping_Angelmaria
    ```
    
2. Crear y activar un entorno virtual:
    
    ```
    python -m venv env
    source env/bin/activate  # En Windows: env\\Scripts\\activate
    ```
    
3. Instalar las dependencias:
    
    ```
    pip install -r requirements.txt
    ```
    

### Pasos de instalaci√≥n

1. Configurar la base de datos en `settings.py`
2. Aplicar las migraciones:
    
    ```
    python manage.py migrate
    ```
    
3. Crear un superusuario (solo si se quiere acceder a la zona admin):
    
    ```
    python manage.py createsuperuser
    ```
    

### Ejecuci√≥n del proyecto localmente

1. Iniciar el servidor de desarrollo:
    
    ```
    python manage.py runserver
    ```
    
2. Acceder a la aplicaci√≥n en `http://localhost:8000`
3. Acceder al panel de administraci√≥n en `http://localhost:8000/admin`

### Configuraci√≥n inicial

1. Ejecutar el archivo update_quotes.py que automatiza la actualizaci√≥n del ‚Äúscraping‚Äù y ejecuta tanto el scraper inicial (web_scarper) como la importaci√≥n de los datos iniciales (data_import):

```
python manage.py update_quotes
```

## 5. Componentes Principales

### 5.1 Web Scraper (web_scraper.py)

- Prop√≥sito: Extraer citas e informaci√≥n de autores de [quotes.toscrape.com](http://quotes.toscrape.com/)
- Funcionalidad:
    - Utiliza las bibliotecas `requests` para hacer peticiones HTTP y `BeautifulSoup` para analizar el HTML
    - Navega por m√∫ltiples p√°ginas del sitio web (hasta 5 p√°ginas por defecto)
    - Extrae citas, autores y etiquetas de cada p√°gina
    - Recopila informaci√≥n detallada del autor de sus p√°ginas individuales
- M√©todos clave:
    - `scrape_quotes()`: M√©todo principal que coordina el proceso de scraping
    - `scrape_author_info(author_url)`: Extrae informaci√≥n detallada del autor
    - `convert_date(date_str)`: Convierte fechas al formato YYYY-MM-DD
- Manejo de errores:
    - Utiliza bloques try/except para manejar errores de red y HTTP
    - Registra advertencias para fechas no reconocidas o informaci√≥n faltante del autor

### 5.2 Importaci√≥n de Datos (data_import.py)

- Prop√≥sito: Importar los datos extra√≠dos a la base de datos de Django
- Funcionalidad:
    - Lee el archivo JSON generado por el web scraper
    - Crea o actualiza registros en la base de datos para autores, citas y etiquetas
- Proceso de importaci√≥n:
    - Itera sobre los autores en el JSON, creando o actualizando registros de Author
    - Para cada cita, crea un registro de Quote y lo asocia con su autor
    - Crea registros de Tag y los asocia con las citas correspondientes a trav√©s de QuoteTag

### 5.3 Mecanismo de Actualizaci√≥n (update_quotes.py)

- Prop√≥sito: Automatizar la actualizaci√≥n peri√≥dica de la base de datos
- Funcionalidad:
    - Utiliza la biblioteca `schedule` para programar actualizaciones diarias
    - Ejecuta el web scraper y el proceso de importaci√≥n de datos
- M√©todos clave:
    - `schedule_jobs()`: Configura la tarea para ejecutarse diariamente a medianoche
    - `run_update()`: Ejecuta el proceso de actualizaci√≥n completo

### 5.4 Vistas y Plantillas

- quote_list.html: Muestra una lista paginada de citas con opciones de filtrado
- author_detail.html: Muestra informaci√≥n detallada sobre un autor y sus citas
- tag_detail.html: Muestra citas asociadas a una etiqueta espec√≠fica
- about.html: Proporciona informaci√≥n sobre el proyecto
- base.html: Plantilla base que define la estructura com√∫n de las p√°ginas

## 6. Modelos de Datos

### Modelo de Author

- Campos:
    - `name`: CharField, nombre del autor
    - `born`: DateField, fecha de nacimiento del autor
    - `birth_place`: CharField, lugar de nacimiento
    - `about`: TextField, biograf√≠a del autor
    - `about_page_url`: URLField, enlace a la p√°gina de informaci√≥n del autor

### Modelo de Quote

- Campos:
    - `text`: TextField, texto de la cita
    - `author`: ForeignKey a Author, establece la relaci√≥n con el autor

### Modelo de Tag

- Campos:
    - `name`: CharField, nombre de la etiqueta

### Modelo de QuoteTag (Relaci√≥n Many-to-Many)

- Campos:
    - `quote`: ForeignKey a Quote
    - `tag`: ForeignKey a Tag

### Relaciones entre modelos:

- Un Author puede tener m√∫ltiples Quotes (relaci√≥n uno a muchos)
- Una Quote puede tener m√∫ltiples Tags, y un Tag puede estar asociado a m√∫ltiples Quotes (relaci√≥n muchos a muchos a trav√©s de QuoteTag)

Estos modelos permiten una estructura de datos flexible y eficiente para almacenar y recuperar citas, autores y etiquetas, facilitando las operaciones de filtrado y b√∫squeda en la aplicaci√≥n.

## 7. Vistas y URLs (endpoints)

1. Lista de citas (quote_list.html)
    - URL: `/`
    - Vista: `quote_list`
2. Detalle del autor (author_detail.html)
    - URL: `/author/<int:author_id>/`
    - Vista: `author_detail`
3. Detalle de la etiqueta (tag_detail.html)
    - URL: `/tag/<int:tag_id>/`
    - Vista: `tag_detail`
4. P√°gina "Acerca de" (about.html)
    - URL: `/about/`
    - Vista: `about`
5. Plantilla base (base.html)
    - Esta es una plantilla base que es extendida por las otras plantillas (con los elementos comunes como la barra de navegaci√≥n y la carga de los css y js).

Adem√°s, hay vistas adicionales para las visualizaciones de datos:

- URL: `/tag-distribution/`
Vista: `generate_tag_distribution_chart`
- URL: `/author-distribution/`
Vista: `generate_author_distribution_chart`
- URL: `/author-decade-distribution/`
Vista: `generate_author_decade_distribution_chart`

## 8. Visualizaci√≥n de Datos

- Gr√°fico de distribuci√≥n de etiquetas

![image](https://github.com/user-attachments/assets/87bae038-9a36-4258-8c5f-57f2d883cfcf)

- Gr√°fico de distribuci√≥n de autores

![image](https://github.com/user-attachments/assets/057ef282-5c65-4ae3-91fa-ea34faf57768)

- Gr√°fico de distribuci√≥n de autores por d√©cada

![image](https://github.com/user-attachments/assets/d1101ba8-d0a4-4ecd-aa76-eed0ba873bcf)

## 9. Test

### Estrategia de pruebas

El proyecto utiliza la biblioteca unittest de Python para realizar pruebas unitarias. Las pruebas se centran en el componente de web scraping, cubriendo diferentes aspectos como la extracci√≥n de datos, el manejo de diferentes formatos y el comportamiento en distintos escenarios.

### Tests implementados

1. `test_scrape_quotes_success`: Verifica la extracci√≥n correcta de citas, autores y etiquetas.
2. `test_scrape_author_info_success`: Comprueba la extracci√≥n de informaci√≥n del autor.
3. `test_convert_date`: Prueba la funci√≥n de conversi√≥n de fechas.
4. `test_scrape_quotes_no_quotes`: Verifica el comportamiento cuando no hay citas para extraer.

### Ejecuci√≥n de pruebas

Para ejecutar todas las pruebas:

```
python manage.py test
```

Para ejecutar un test espec√≠fico:

```
python manage.py test quotes.tests.TestWebScraper.test_scrape_quotes_success
```

### Escribir nuevas pruebas

Al agregar nuevas funcionalidades, se deben escribir pruebas correspondientes. Seguir el patr√≥n existente en el archivo de pruebas:

1. Importar las dependencias necesarias.
2. Crear una nueva funci√≥n de prueba dentro de la clase `TestWebScraper`.
3. Usar mocks para simular respuestas HTTP si es necesario.
4. Verificar el comportamiento esperado usando aserciones.

## 10. Despliegue

### Despliegue con Docker

El proyecto est√° configurado para ser desplegado utilizando Docker y Docker Compose, lo que facilita la configuraci√≥n del entorno y asegura la consistencia entre desarrollo y producci√≥n.

### Prerequisitos

- Docker
- Docker Compose

### Archivos de configuraci√≥n

1. Dockerfile:

```
FROM python:3.9

WORKDIR /app

COPY requirements.txt .
COPY wait-for-it.sh /usr/local/bin/wait-for-it
RUN chmod +x /usr/local/bin/wait-for-it

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["python", "manage.py", "runserver", "0.0.0.0:8000"]
```

1. docker-compose.yml:

```yaml
version: '3.8'

services:
  web:
    build: .
    command: ["wait-for-it", "db:3306", "-t", "60", "--", "python", "manage.py", "runserver", "0.0.0.0:8000"]
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    depends_on:
      - db
    env_file:
      - .env

  db:
    image: mysql:8.0
    environment:
      MYSQL_DATABASE: ${DB_NAME}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    volumes:
      - mysql_data:/var/lib/mysql
    ports:
      - "3307:3306"

volumes:
  mysql_data:
```

### Proceso de despliegue

1. Aseg√∫rate de tener un archivo `.env` en la ra√≠z del proyecto con las variables de entorno necesarias:
    
    ```
    DB_NAME=nombre_de_tu_base_de_datos
    MYSQL_ROOT_PASSWORD=tu_contrase√±a_segura
    
    ```
    
2. Construye las im√°genes de Docker:
    
    ```
    docker-compose build
    ```
    
3. Inicia los contenedores:
    
    ```
    docker-compose up -d
    ```
    
4. Ejecuta las migraciones de Django:
    
    ```
    docker-compose exec -it webscraping_angelmaria-web-1 /bin/bash 
    
    python manage.py migrate
    ```
    
5. Crea un superusuario (opcional):
    
    ```
    docker-compose exec web python manage.py createsuperuser
    ```
    
6. Accede a la aplicaci√≥n en `http://localhost:8000`

### Notas importantes:

- El servicio web espera a que la base de datos est√© lista antes de iniciar, gracias al script `wait-for-it.sh`.
- Los datos de MySQL se persisten en un volumen Docker (`mysql_data`).
- Para producci√≥n, considera cambiar el comando en el Dockerfile para usar Gunicorn en lugar del servidor de desarrollo de Django.

### Mantenimiento y actualizaciones

- Para aplicar cambios en el c√≥digo, construye las imagenes si no existen o si han cambiado, inicia los contenedores y muestra los logs en la consola actual:
    
    ```
    docker-compose up --build
    ```
    
- Para ejecutar comandos de Django (como `collectstatic`), -crucial para el despliegue en producci√≥n- usa:
    
    ```
    docker-compose exec web python manage.py collectstatic
    ```
    

### Monitoreo

- Revisa los logs de los contenedores con:
    
    ```
    docker-compose logs
    
    ```
    
- Para un contenedor espec√≠fico:
    
    ```
    docker-compose logs nombre-del-contenedor
    
    ```
    

Este enfoque de despliegue con Docker proporciona un entorno consistente y f√°cil de configurar, facilitando el desarrollo y el despliegue de la aplicaci√≥n.

## 11. Mantenimiento y Actualizaciones

### Tareas programadas

- La actualizaci√≥n diaria de citas est√° configurada en `update_quotes.py`:
    
    ```python
    def schedule_jobs(self):
        # Schedule the job to run daily at midnight
        schedule.every().day.at("00:00").do(self.run_update)
    ```
    
- Considerar usar un sistema de tareas m√°s robusto como Celery para producci√≥n

### Monitoreo y registro

- Configurar un sistema de logging para registrar errores y actividades importantes
- Utilizar herramientas de monitoreo como Sentry o New Relic para seguimiento en tiempo real

### Estrategia de respaldo

- Configurar copias de seguridad diarias de la base de datos
- Almacenar copias de seguridad en una ubicaci√≥n segura y remota
- Probar regularmente la restauraci√≥n de copias de seguridad

### Actualizaciones de dependencias

- Revisar y actualizar regularmente las dependencias del proyecto
- Probar exhaustivamente despu√©s de cada actualizaci√≥n

## 12. Soluci√≥n de Problemas

### Problemas comunes y sus soluciones

- Error de conexi√≥n al scraper: Verificar la conectividad a internet y el estado del sitio web objetivo
- Errores de base de datos: Comprobar la conexi√≥n y los permisos de la base de datos
- Errores de importaci√≥n de m√≥dulos: Verificar que todas las dependencias est√©n instaladas correctamente

### Consejos de depuraci√≥n

- Utilizar logging extensivo para rastrear el flujo de ejecuci√≥n
- Usar herramientas de depuraci√≥n de Django como Django Debug Toolbar
- Configurar alertas para errores cr√≠ticos

## 13. Mejoras Futuras

- Implementar autenticaci√≥n de usuarios para personalizar la experiencia
- A√±adir funcionalidad de b√∫squeda avanzada de citas
- Mejorar las visualizaciones de datos con gr√°ficos interactivos
- Desplegar la aplicaci√≥n en una plataforma cloud como AWS Elastic Beanstalk o Google Cloud Platform para mejorar la escalabilidad y confiabilidad

## 14. Contribuci√≥n

### Pautas para contribuir al proyecto

- Fork el repositorio y cree una nueva rama para su funci√≥n o correcci√≥n
- Siga las convenciones de c√≥digo existentes
- Escriba pruebas para cualquier nueva funcionalidad
- Aseg√∫rese de que todas las pruebas pasen antes de enviar un pull request
- Documente cualquier cambio en el README o la documentaci√≥n relevante

### Proceso de pull request

1. Crear un fork del repositorio
2. Clonar su fork localmente
3. Crear una nueva rama para su funci√≥n
4. Hacer cambios y commit
5. Empujar los cambios a su fork
6. Crear un pull request desde su fork al repositorio principal

## 15. Licencia

Este proyecto est√° licenciado bajo la Licencia MIT. Consulte el archivo `LICENSE` en el repositorio para obtener el texto completo de la licencia.

La Licencia MIT es una licencia de software permisiva que permite la reutilizaci√≥n del software dentro de software propietario siempre que se incluya el texto de la licencia. Es compatible con muchas licencias copyleft, como la GNU General Public License.

## 16. Informaci√≥n de Contacto

[Perfil de Linkedin](https://www.linkedin.com/in/angelmariamartinez/)

[Documentaci√≥n en Notion](https://beaded-sociology-706.notion.site/Web-Scraping-Proyecto-Individual-686701db4ac140aabaa975fde3233755)

